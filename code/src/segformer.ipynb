{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input,\n",
    "           size=None,\n",
    "           scale_factor=None,\n",
    "           mode='nearest',\n",
    "           align_corners=None):\n",
    "    if isinstance(size, torch.Size):\n",
    "        size = tuple(int(x) for x in size)\n",
    "    return F.interpolate(input, size, scale_factor, mode, align_corners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        stride: int,\n",
    "        in_channels: int = 4,\n",
    "        embedding_dim: int = 768\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.embeddings_projection = nn.Conv2d(in_channels=in_channels,\n",
    "                                               out_channels=embedding_dim,\n",
    "                                               kernel_size=patch_size,\n",
    "                                               stride=stride,\n",
    "                                               padding=(patch_size // 2, patch_size // 2))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, H, W)\n",
    "        Returns:\n",
    "            x' : (B, H' * W', embedding_dim)\n",
    "            num_patches_H: int\n",
    "            num_patches_W: int\n",
    "        \"\"\"\n",
    "        x = self.embeddings_projection(x) # (B, embedding_dim, H', W')\n",
    "        \n",
    "        num_patches_H = x.shape[2]\n",
    "        num_patches_W = x.shape[3]\n",
    "        \n",
    "        x = x.flatten(start_dim=2, end_dim=-1) # (B, embedding_dim, H' * W')\n",
    "        \n",
    "        x = self.reshape_for_layer_norm(x) # (B, H' * W', embedding_dim)\n",
    "        \n",
    "        x = self.layer_norm(x) # (B, H' * W', embedding_dim)\n",
    "        \n",
    "        return x, num_patches_H, num_patches_W\n",
    "\n",
    "    def reshape_for_layer_norm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed1 = OverlapPatchEmbeddings(image_size=512,\n",
    "                                      patch_size=7,\n",
    "                                      stride=4,\n",
    "                                      in_channels=4,\n",
    "                                      embedding_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = torch.randn(2, 4, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16384, 64])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed1(img_batch)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = False,\n",
    "        attention_dropout: float = 0.,\n",
    "        projection_dropout: float = 0.,\n",
    "        reduction_ratio: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Based on original paper (Attention is all you need), d_model must be divisible by num_heads\n",
    "        assert d_model % num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads}) !\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (d_model // num_heads) ** -0.5 # scale factor from original paper (Attention is all you need)\n",
    "        \n",
    "        self.q = nn.Linear(in_features=d_model,\n",
    "                           out_features=d_model,\n",
    "                           bias=qkv_bias)\n",
    "        self.k = nn.Linear(in_features=d_model,\n",
    "                           out_features=d_model,\n",
    "                           bias=qkv_bias)\n",
    "        self.v = nn.Linear(in_features=d_model,\n",
    "                           out_features=d_model,\n",
    "                           bias=qkv_bias)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(p=attention_dropout)\n",
    "        \n",
    "        self.projection = nn.Linear(in_features=d_model,\n",
    "                                    out_features=d_model)\n",
    "        self.projection_dropout = nn.Dropout(p=projection_dropout)\n",
    "        \n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        if reduction_ratio > 1:\n",
    "            self.reduction = nn.Conv2d(in_channels=d_model,\n",
    "                                       out_channels=d_model,\n",
    "                                       kernel_size=reduction_ratio,\n",
    "                                       stride=reduction_ratio)\n",
    "            self.reduction_layer_norm = nn.LayerNorm(normalized_shape=d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, C)\n",
    "            H: int\n",
    "            W: int\n",
    "        Returns:\n",
    "            x: (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        queries = self.q(x) # (B, N, C)\n",
    "        queries = queries.reshape(B, N, self.num_heads, C // self.num_heads) # (B, N, num_heads, C // num_heads)\n",
    "        queries = queries.permute(0, 2, 1, 3) # (B, num_heads, N, C // num_heads)\n",
    "        \n",
    "        if self.reduction_ratio > 1:\n",
    "            x_reshaped = x.permute(0, 2, 1).reshape(B, C, H, W) # (B, C, H, W)\n",
    "            x_reduced = self.reduction(x_reshaped).reshape(B, C, -1).permute(0, 2, 1) # (B, N_reduced, C)\n",
    "            x_reduced = self.reduction_layer_norm(x_reduced) # (B, N_reduced, C)\n",
    "            \n",
    "            keys = self.k(x_reduced) # (B, N_reduced, C)\n",
    "            keys = keys.reshape(B, -1, self.num_heads, C // self.num_heads) # (B, N_reduced, num_heads, C // num_heads)\n",
    "            keys = keys.permute(0, 2, 1, 3) # (B, num_heads, N_reduced, C // num_heads)\n",
    "            \n",
    "            values = self.v(x_reduced) # (B, N_reduced, C)\n",
    "            values = values.reshape(B, -1, self.num_heads, C // self.num_heads) # (B, N_reduced, num_heads, C // num_heads)\n",
    "            values = values.permute(0, 2, 1, 3) # (B, num_heads, N_reduced, C // num_heads)\n",
    "        else:\n",
    "            keys = self.k(x) # (B, N, C)\n",
    "            keys = keys.reshape(B, -1, self.num_heads, C // self.num_heads) # (B, N, num_heads, C // num_heads)\n",
    "            keys = keys.permute(0, 2, 1, 3) # (B, num_heads, N, C // num_heads)\n",
    "            \n",
    "            values = self.v(x) # (B, N, C)\n",
    "            values = values.reshape(B, -1, self.num_heads, C // self.num_heads) # (B, N, num_heads, C // num_heads)\n",
    "            values = values.permute(0, 2, 1, 3) # (B, num_heads, N, C // num_heads)\n",
    "        \n",
    "        keys = keys.transpose(-2, -1) # (B, num_heads, C // num_heads, N)\n",
    "        \n",
    "        attention = queries.matmul(keys) * self.scale # (B, num_heads, N, N)\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.attention_dropout(attention)\n",
    "        \n",
    "        x = attention.matmul(values).transpose(1, 2).reshape(B, N, C) # (B, N, C)\n",
    "        x = self.projection(x)\n",
    "        x = self.projection_dropout(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficient_self_attention = EfficientSelfAttention(d_model=64,\n",
    "                                                  num_heads=8,\n",
    "                                                  reduction_ratio=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3136, 64])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficient_self_attention(torch.randn(10, (56*56), 64), 56, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Taken from: https://github.com/NVlabs/SegFormer/tree/master\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=768):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dwconv = nn.Conv2d(in_channels=dim,\n",
    "                                out_channels=dim, \n",
    "                                kernel_size=3,\n",
    "                                stride=1,\n",
    "                                padding=1,\n",
    "                                bias=True,\n",
    "                                groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        x = self.dwconv(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixFFN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        dropout: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        hidden_features = hidden_features or in_features\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=in_features,\n",
    "                             out_features=hidden_features)\n",
    "        \n",
    "        self.conv = DWConv(dim=hidden_features)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=hidden_features,\n",
    "                             out_features=out_features)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, C)\n",
    "            H: int\n",
    "            W: int\n",
    "        Returns:\n",
    "            x: (B, N, C)\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.conv(x, H, W)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MixFFN(in_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3136, 64])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(torch.randn(10, (56*56), 64), 56, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: int = 4,\n",
    "        qkv_bias: bool = False,\n",
    "        attention_dropout: float = 0.,\n",
    "        dropout: float = 0.,\n",
    "        reduction_ratio: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        \n",
    "        self.attention = EfficientSelfAttention(d_model=d_model,\n",
    "                                                num_heads=num_heads,\n",
    "                                                qkv_bias=qkv_bias,\n",
    "                                                attention_dropout=attention_dropout,\n",
    "                                                projection_dropout=dropout,\n",
    "                                                reduction_ratio=reduction_ratio)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=d_model)\n",
    "        \n",
    "        mlp_hidden_dim = int(d_model * mlp_ratio)\n",
    "        self.mix_ffn = MixFFN(in_features=d_model,\n",
    "                              hidden_features=mlp_hidden_dim,\n",
    "                              dropout=dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, N, C)\n",
    "            H: int\n",
    "            W: int\n",
    "        Returns:\n",
    "            x: (B, N, C)\n",
    "        \"\"\"\n",
    "        x = x + self.attention(self.norm1(x), H, W)\n",
    "        x = x + self.mix_ffn(self.norm2(x), H, W)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block = TransformerBlock(d_model=64,\n",
    "                                     num_heads=8,\n",
    "                                     mlp_ratio=4,\n",
    "                                     reduction_ratio=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3136, 64])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_block(torch.randn(10, (56*56), 64), 56, 56).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixVisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        in_channels: int = 4,\n",
    "        num_classes: int = 1,\n",
    "        embedding_dims: list = [64, 128, 256, 512],\n",
    "        num_heads: list = [1, 2, 4, 8],\n",
    "        mlp_ratios: list = [4, 4, 4, 4],\n",
    "        qkv_bias: bool = False,\n",
    "        attention_dropout: float = 0.,\n",
    "        dropout: float = 0.,\n",
    "        reduction_ratios: list = [8, 4, 2, 1],\n",
    "        depths: list = [3, 4, 6, 3]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "        \n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size=image_size,\n",
    "                                                   patch_size=7,\n",
    "                                                   stride=4,\n",
    "                                                   in_channels=in_channels,\n",
    "                                                   embedding_dim=embedding_dims[0])\n",
    "        \n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size=image_size // 4,\n",
    "                                                   patch_size=3,\n",
    "                                                   stride=2,\n",
    "                                                   in_channels=embedding_dims[0],\n",
    "                                                   embedding_dim=embedding_dims[1])\n",
    "        \n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size=image_size // 8,\n",
    "                                                   patch_size=3,\n",
    "                                                   stride=2,\n",
    "                                                   in_channels=embedding_dims[1],\n",
    "                                                   embedding_dim=embedding_dims[2])\n",
    "        \n",
    "        self.patch_embed4 = OverlapPatchEmbeddings(image_size=image_size // 16,\n",
    "                                                   patch_size=3,\n",
    "                                                   stride=2,\n",
    "                                                   in_channels=embedding_dims[2],\n",
    "                                                   embedding_dim=embedding_dims[3])\n",
    "        \n",
    "        self.block1 = nn.ModuleList([\n",
    "            TransformerBlock(d_model=embedding_dims[0],\n",
    "                             num_heads=num_heads[0],\n",
    "                             mlp_ratio=mlp_ratios[0],\n",
    "                             qkv_bias=qkv_bias,\n",
    "                             attention_dropout=attention_dropout,\n",
    "                             dropout=dropout,\n",
    "                             reduction_ratio=reduction_ratios[0]) for _ in range(depths[0])\n",
    "        ])\n",
    "        self.norm1 = nn.LayerNorm(normalized_shape=embedding_dims[0])\n",
    "        \n",
    "        self.block2 = nn.ModuleList([\n",
    "            TransformerBlock(d_model=embedding_dims[1],\n",
    "                             num_heads=num_heads[1],\n",
    "                             mlp_ratio=mlp_ratios[1],\n",
    "                             qkv_bias=qkv_bias,\n",
    "                             attention_dropout=attention_dropout,\n",
    "                             dropout=dropout,\n",
    "                             reduction_ratio=reduction_ratios[1]) for _ in range(depths[1])\n",
    "        ])\n",
    "        self.norm2 = nn.LayerNorm(normalized_shape=embedding_dims[1])\n",
    "        \n",
    "        self.block3 = nn.ModuleList([\n",
    "            TransformerBlock(d_model=embedding_dims[2],\n",
    "                             num_heads=num_heads[2],\n",
    "                             mlp_ratio=mlp_ratios[2],\n",
    "                             qkv_bias=qkv_bias,\n",
    "                             attention_dropout=attention_dropout,\n",
    "                             dropout=dropout,\n",
    "                             reduction_ratio=reduction_ratios[2]) for _ in range(depths[2])\n",
    "        ])\n",
    "        self.norm3 = nn.LayerNorm(normalized_shape=embedding_dims[2])\n",
    "        \n",
    "        self.block4 = nn.ModuleList([\n",
    "            TransformerBlock(d_model=embedding_dims[3],\n",
    "                             num_heads=num_heads[3],\n",
    "                             mlp_ratio=mlp_ratios[3],\n",
    "                             qkv_bias=qkv_bias,\n",
    "                             attention_dropout=attention_dropout,\n",
    "                             dropout=dropout,\n",
    "                             reduction_ratio=reduction_ratios[3]) for _ in range(depths[3])\n",
    "        ])\n",
    "        self.norm4 = nn.LayerNorm(normalized_shape=embedding_dims[3])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.shape[0]\n",
    "        outputs = []\n",
    "        \n",
    "        x = self.compute_stage_1(x, B)\n",
    "        outputs.append(x)\n",
    "        \n",
    "        x = self.compute_stage_2(x, B)\n",
    "        outputs.append(x)\n",
    "        \n",
    "        x = self.compute_stage_3(x, B)\n",
    "        outputs.append(x)\n",
    "        \n",
    "        x = self.compute_stage_4(x, B)\n",
    "        outputs.append(x)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def compute_stage_1(self, x: torch.Tensor, B: int) -> torch.Tensor:\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        \n",
    "        for transformer_block in self.block1:\n",
    "            x = transformer_block(x, H, W)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_stage_2(self, x: torch.Tensor, B: int) -> torch.Tensor:\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        \n",
    "        for transformer_block in self.block2:\n",
    "            x = transformer_block(x, H, W)\n",
    "        \n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_stage_3(self, x: torch.Tensor, B: int) -> torch.Tensor:\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        \n",
    "        for transformer_block in self.block3:\n",
    "            x = transformer_block(x, H, W)\n",
    "        \n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def compute_stage_4(self, x: torch.Tensor, B: int) -> torch.Tensor:\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        \n",
    "        for transformer_block in self.block4:\n",
    "            x = transformer_block(x, H, W)\n",
    "        \n",
    "        x = self.norm4(x)\n",
    "        \n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit = MixVisionTransformer(image_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "mit_outputs = mit(torch.randn(10, 4, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 128, 128])\n",
      "torch.Size([10, 128, 64, 64])\n",
      "torch.Size([10, 256, 32, 32])\n",
      "torch.Size([10, 512, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for output in mit_outputs:\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=input_dim,\n",
    "                            out_features=embedding_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, input_dim, H, W)\n",
    "        Returns:\n",
    "            x: (B, H * W, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = x.flatten(start_dim=2, end_dim=-1).transpose(1, 2) # (B, H * W, C)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_dim=512, embedding_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 768])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(torch.randn(10, 512, 16, 16)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormerSemanticSegmentationHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        c1_in_channels: int,\n",
    "        c2_in_channels: int,\n",
    "        c3_in_channels: int,\n",
    "        c4_in_channels: int,\n",
    "        num_classes: int,\n",
    "        embedding_dim: int = 768,\n",
    "        dropout: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.linear_c4 = MLP(input_dim=c4_in_channels, embedding_dim=embedding_dim)\n",
    "        self.linear_c3 = MLP(input_dim=c3_in_channels, embedding_dim=embedding_dim)\n",
    "        self.linear_c2 = MLP(input_dim=c2_in_channels, embedding_dim=embedding_dim)\n",
    "        self.linear_c1 = MLP(input_dim=c1_in_channels, embedding_dim=embedding_dim)\n",
    "        \n",
    "        number_of_blocks = 4\n",
    "        self.linear_fuse_conv = nn.Conv2d(in_channels=embedding_dim * number_of_blocks,\n",
    "                                          out_channels=embedding_dim,\n",
    "                                          kernel_size=1)\n",
    "        self.linear_fuse_norm = nn.SyncBatchNorm(num_features=embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.out = nn.Conv2d(in_channels=embedding_dim, out_channels=num_classes, kernel_size=1)\n",
    "    \n",
    "    def apply_linear_fuse(self, c1: torch.Tensor, c2: torch.Tensor, c3: torch.Tensor, c4: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.linear_fuse_conv(torch.cat([c4, c3, c2, c1], dim=1))\n",
    "        output = self.linear_fuse_norm(output)\n",
    "        output = F.relu(output)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, inputs: list) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: list of tensors from the output of the MixVisionTransformer\n",
    "        Returns:\n",
    "            x: (B, H * W, num_classes)\n",
    "        \"\"\"\n",
    "        c1, c2, c3, c4 = inputs\n",
    "        \n",
    "        n, _, h, w = c4.shape\n",
    "\n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "        _c4 = resize(_c4, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c3 = self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3])\n",
    "        _c3 = resize(_c3, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c2 = self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3])\n",
    "        _c2 = resize(_c2, size=c1.size()[2:],mode='bilinear',align_corners=False)\n",
    "\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "\n",
    "        _c = self.apply_linear_fuse(c1=_c1, c2=_c2, c3=_c3, c4=_c4)\n",
    "\n",
    "        x = self.dropout(_c)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = SegFormerSemanticSegmentationHead(c1_in_channels=64,\n",
    "                                         c2_in_channels=128,\n",
    "                                         c3_in_channels=256,\n",
    "                                         c4_in_channels=512,\n",
    "                                         num_classes=1,\n",
    "                                         embedding_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 128])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(mit_outputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegFormer Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int = 512,\n",
    "        num_classes: int = 1,\n",
    "        in_channels: int = 4,\n",
    "        encoder_embedding_dims: list = [64, 128, 256, 512],\n",
    "        decoder_embedding_dim: int = 768,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.encoder_embedding_dims = encoder_embedding_dims\n",
    "        self.decoder_embedding_dim = decoder_embedding_dim\n",
    "        \n",
    "        self.encoder = MixVisionTransformer(image_size=image_size,\n",
    "                                             in_channels=in_channels,\n",
    "                                             num_classes=num_classes,\n",
    "                                             embedding_dims=encoder_embedding_dims,\n",
    "                                             num_heads=[1, 2, 4, 8],\n",
    "                                             mlp_ratios=[4, 4, 4, 4],\n",
    "                                             qkv_bias=False,\n",
    "                                             attention_dropout=0.5,\n",
    "                                             dropout=0.5,\n",
    "                                             reduction_ratios=[8, 4, 2, 1],\n",
    "                                             depths=[3, 4, 6, 3])\n",
    "        \n",
    "        self.decoder = SegFormerSemanticSegmentationHead(c1_in_channels=encoder_embedding_dims[0],\n",
    "                                                         c2_in_channels=encoder_embedding_dims[1],\n",
    "                                                         c3_in_channels=encoder_embedding_dims[2],\n",
    "                                                         c4_in_channels=encoder_embedding_dims[3],\n",
    "                                                         num_classes=num_classes,\n",
    "                                                         embedding_dim=decoder_embedding_dim,\n",
    "                                                         dropout=0.5)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return resize(input=x, size=(self.image_size, self.image_size), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "segformer = SegFormer(image_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 512, 512])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segformer(torch.randn(10, 4, 512, 512)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
